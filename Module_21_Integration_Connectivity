21.1-Calling one notebook from another
# Step1- create empdf in first notebook

# Create the Employee DataFrame
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Define schema for Employee DataFrame
emp_schema = StructType([
    StructField("emp_id", IntegerType(), True),
    StructField("emp_name", StringType(), True),
    StructField("dept_id", IntegerType(), True)
])

# Sample employee data
emp_data = [
    (1, "Alice", 101),
    (2, "Bob", 102),
    (3, "Charlie", 101),
    (4, "David", 103)
]

# Create Employee DataFrame
emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)

# Display the Employee DataFrame
emp_df.show()

# Register as a temporary view for SQL or share with another notebook
#emp_df.createOrReplaceTempView("Employee")
----------------------
# Step 2- Create deptdf in second notebook

# Create the Department DataFrame

from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Define schema for Department DataFrame
dept_schema = StructType([
    StructField("dept_id", IntegerType(), True),
    StructField("dept_name", StringType(), True)
])

# Sample department data
dept_data = [
    (101, "HR"),
    (102, "Engineering"),
    (103, "Marketing")
]

# Create Department DataFrame
dept_df = spark.createDataFrame(data=dept_data, schema=dept_schema)

# Display the Department DataFrame
dept_df.show()

# Register as a temporary view for SQL or share with another notebook
#dept_df.createOrReplaceTempView("Department")
----------------------------------
# Step3- execute the run command and then join command
%run "./notebook1"

join_table = emp_df.join(dept_df, emp_df.dept_id == dept_df.dept_id, how = "inner")
display(join_table)
---------------------------
# Run Notebook1_Producer and Notebook2_Consumer
# Note- This will work only in Enterprise Edition
